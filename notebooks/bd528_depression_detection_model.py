# -*- coding: utf-8 -*-
"""BD528_Depression Detection Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17hhwRA6ztKjXvCQ54F79uA6pOiLgOY_x

##Import
"""

# import pandas as pd
# import numpy as np
# import matplotlib as plt
# import seaborn as sns

# import tensorflow as tf
# import keras

# from keras.preprocessing.text import Tokenizer
# from keras.preprocessing.sequence import pad_sequences
# from keras.models import Model, Sequential
# from keras.layers import GRU, Input, Dense, Activation, RepeatVector, Bidirectional, LSTM, Dropout, Embedding
# from keras.layers.embeddings import Embedding
# from sklearn.model_selection import train_test_split
# from keras.losses import sparse_categorical_crossentropy
# from keras.preprocessing.text import Tokenizer
# from keras.preprocessing import sequence
# from keras.callbacks import EarlyStopping

# from sklearn.model_selection import train_test_split
# from sklearn.metrics import confusion_matrix, classification_report
# import collections

# from tensorflow.python.client import device_lib
# import matplotlib.pyplot as plt
# import seaborn as sns


# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
# SEED = 10

import pandas as pd
import numpy as np
import matplotlib as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import GRU, Input, Dense, Activation, RepeatVector, Bidirectional, LSTM, Dropout, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.losses import sparse_categorical_crossentropy

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import collections

import matplotlib.pyplot as plt
import seaborn as sns

tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
SEED = 10

# from google.colab import drive
# drive.mount('/content/drive')

"""Pulling data from Twitter API"""

url = 'https://drive.google.com/uc?export=download&id=19-O6-b7PLX3uHmfoLAvVdO0OqF8xsn9v'
df = pd.read_csv(url)

# df = pd.read_csv('../input/mental-health-social-media/Mental-Health-Twitter.csv')

X = df['post_text']
y = df['label']

"""##EDA"""

plt.figure(figsize = (10,6))
sns.countplot(x = df['label'], palette = 'Set1', alpha = 0.8)
plt.title('Distribution of Target')

df['num_words'] = df['post_text'].apply(lambda x: len(x.split()))
plt.figure(figsize = (20,6))
sns.histplot(df['num_words'], bins = range(1, 40, 2), palette = 'Set1', alpha = 0.8)
plt.title('Distribution of the Word Count')

"""##Preparing Input"""

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = SEED)

"""Tokenization"""

#define Keras Tokenizer
tok = Tokenizer()
tok.fit_on_texts(X_train)

#return sequences
sequences = tok.texts_to_sequences(X_train)
test_sequences = tok.texts_to_sequences(X_test)

#print size of the vocabulary
print(f'Train vocabulary size: {len(tok.word_index)}')

"""Pad sequences to fixed lenght"""

#maximum sequence length (512 to prevent memory issues and speed up computation)
MAX_LEN = 40

#padded sequences
# X_train_seq = sequence.pad_sequences(sequences,maxlen=MAX_LEN)
# X_test_seq = sequence.pad_sequences(test_sequences,maxlen=MAX_LEN)
X_train_seq = pad_sequences(sequences,maxlen=MAX_LEN) # Call pad_sequences directly
X_test_seq = pad_sequences(test_sequences,maxlen=MAX_LEN) # Call pad_sequences directly

"""##Train and evaluate the Keras LSTM model

Define the model
"""

vocab_size = len(tok.word_index) + 1

# Redefine the model with the adjusted vocabulary size
model = tf.keras.Sequential([
    Input(name='inputs', shape=[MAX_LEN]),
    Embedding(vocab_size, 128),  # Use the adjusted vocab_size
    Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    Bidirectional(tf.keras.layers.LSTM(32)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

# Now, you should be able to fit the model without the error:
history = model.fit(X_train_seq, y_train, epochs=10,
                    validation_split=0.2, batch_size=64,
                    callbacks=[EarlyStopping(monitor='val_accuracy', mode='max', patience=3,
                                             verbose=False, restore_best_weights=True)])

"""Evaluate Model"""

test_loss, test_acc = model.evaluate(X_test_seq, y_test)
y_hat = model.predict(X_test_seq)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)

## print classification report
print(classification_report(y_test, np.where(y_hat >= 0.5, 1, 0)))

#plot the confusion matrix
plt.figure(figsize = (8,6))

sns.heatmap(confusion_matrix(y_test, np.where(y_hat >= 0.5, 1, 0)), annot=True,
            fmt='', cmap='Blues')

plt.xlabel('Predicted Labels')
plt.ylabel('Real Labels')